By now, you have used the intrinsic
evaluation, your first method for evaluating word embeddings based on
how well they capture the semantic or syntactic relationships between the words. To evaluate word embeddings
with extrinsic evaluation, you use the word embeddings to perform an
external task, which is typically the real world task that you initially
needed the word embeddings for. Then, use the performance metric
of this task as a proxy for the quality of the word embeddings. Examples of useful word level tasks
include named entity recognition or parts-of-speech tagging. A named entity is something that can
be referred to by a proper name. For example, in the sentence
Andrew works at deeplearning.ai, the word Andrew is a named entity and
is categorized as a person. The word deeplearning.ai is
another named entity, and is categorized as an organization. A useful task may be to train a model
that can help you identify and categorize named entities in a sentence. You could then evaluate this classifier
on the test set with some selected evaluation metric,
such as accuracy or the F1 score. The classifier's performance on
the evaluation metric represents the combined performance of both
the embedding and classification tasks. Extrinsic evaluation methods
are the ultimate test to ensure that word embeddings
are actually useful. However, their main drawbacks are,
that the evaluation will be more time-consuming than
an intrinsic evaluation. And that if the performance is poor,
then the performance metrics provides no information as to which specific parts of
the end to end process is responsible. The word embeddings themselves
are the external tasks used to test them. So far, you've evaluated word
embeddings using intrinsic and extrinsic evaluation methods,
that's very nice. If you want to dive deeper into this
topic, I've included the reference to a comprehensive and very readable
paper on evaluating word embeddings. You've just acquired quite a few new
skills, it's a good time to review. So that's up next.